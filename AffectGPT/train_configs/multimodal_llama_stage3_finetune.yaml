model:
  arch: video_llama
  model_type: pretrain_vicuna

  # Audio Q-Former and Video Q-Former
  frozen_llama_proj: False
  frozen_video_Qformer: False
  frozen_audio_Qformer: False
  frozen_audio_llama_proj: False

  # Video-LLaMA
  ckpt:   'models/finetune_vicuna7b_videobranch.pth'
  ckpt_2: 'models/finetune_vicuna7b_audiobranch.pth'

  # Other pre-trained models
  llama_model: 'models/vicuna-7b-v0'
  q_former_model: "models/blip2_pretrained_flant5xxl.pth"
  imagebind_ckpt_path: "models/imagebind_huge.pth"


# max_length < 2048
# max_length=512, most samples satisty this condition
# max_length=320, few samples  satisty this condition
datasets:
  cc_sbu_align:
    data_type: image
    build_info:
      ann_path: instruction-dataset/MiniGPT-4/filter_cap.json
      vis_root: instruction-dataset/MiniGPT-4/image
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    num_audio_query_token: 8
    num_video_query_token: 32
    tokenizer_name: "models/vicuna-7b-v0"
    max_length: 512

  llava_instruct:
    data_type: image
    build_info:
      ann_path: instruction-dataset/LLaVA/llava_instruct_150k.json
      vis_root: instruction-dataset/LLaVA/COCO/train2014/
    vis_processor:
      train:
        name: "blip2_image_train"
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    num_audio_query_token: 8
    num_video_query_token: 32
    tokenizer_name: "models/vicuna-7b-v0"
    max_length: 512

  emotion_reason_instruct:
    data_type: video
    build_info:
      ann_path: instruction-dataset/EmoReason/gt-eng.csv
      vis_root: instruction-dataset/EmoReason/video-process
    vis_processor:
      train:
        name: "alpro_video_train"
        n_frms: 8
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    num_audio_query_token: 8
    num_video_query_token: 32
    tokenizer_name: "models/vicuna-7b-v0"
    max_length: 512

  webvid_instruct:
    data_type: video
    build_info:
      ann_path: instruction-dataset/VideoChat/videochat_instruct_11k.json
      vis_root: instruction-dataset/VideoChat/data/videos
    vis_processor:
      train:
        name: "alpro_video_train"
        n_frms: 8
        image_size: 224
    text_processor:
      train:
        name: "blip_caption"
    num_audio_query_token: 8
    num_video_query_token: 32
    tokenizer_name: "models/vicuna-7b-v0"
    max_length: 512


run:
  task: video_text_pretrain

  # optimizer
  lr_sched: "linear_warmup_cosine_lr"
  init_lr: 3e-5
  min_lr: 1e-5
  warmup_lr: 1e-6
  weight_decay: 0.05

  max_epoch: 100
  iters_per_epoch: 1000
  warmup_steps: 1000

  batch_size_train: 2
  batch_size_eval: 2

  seed: 42
  num_workers: 4

  amp: True # auto mixed precision, multiplication (fp16), addition (fp32)
  resume_ckpt_path: null # continue training from resume_ckpt_path

  evaluate: False 
  train_splits: ["train"]

  device: "cuda" 
  world_size: 1
  dist_url: "env://"
  distributed: True